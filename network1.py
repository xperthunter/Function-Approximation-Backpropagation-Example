{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# network.py\n",
    "# ~~~~~~~~~~\n",
    "#\n",
    "# Code directly from Michael Nielsen - \"Neural Networks and Deep Learning\"\n",
    "# This is the module to implement stochastic gradient descent. \n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        # sizes is a list/array that contains number of neurons per layer. \n",
    "        # Example: sizes = [2, 3, 1]. Three layers, first with 2 neurons ...\n",
    "        # The first layer is assumed to be an input layer. \n",
    "        # The weights and biases are initialized randomly from Standard Normal here. \n",
    "        # The input layer has no biases, just layers we are feed-forwarding into. \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        # weight matrix dimension = next layer neurons, previous layer neurons\n",
    "        # its sizes[:-1] because that contains previous layer, in other words, not last layer\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                       for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        # returns the output of network given `a` as the inputs\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "            # np.dot will do dot product when vectors, else inner product. so this will be in general an inner product.\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "           test_data=None):\n",
    "        # Here we are doing the stochastic gradient descent using mini-batches. \n",
    "        # If test_data is provided (different from training data) the partial \n",
    "        # progress at each epoch will be evaluated. Default is none. \n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0,n,mini_batch_size)\n",
    "            ]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                j, self.evaluate(test_data), n_test)\n",
    "            else: \n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        # update the network's weights and biases by applying gradient\n",
    "        # descent per batch. \n",
    "        delta_b_batchsum = [np.zeros(b.shape) for b in self.biases]\n",
    "        delta_w_batchsum = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_b_single, delta_w_single = self.backprop(x, y)\n",
    "            delta_b_batchsum = [nb+dnb for nb, dnb in zip(delta_b_batchsum, delta_b_single)]\n",
    "            delta_w_batchsum = [nw+dnw for nw, dnw in zip(delta_w_batchsum, delta_w_single)]\n",
    "        self.weights = [w - (eta/len(mini_batch))*nw\n",
    "                       for w, nw in zip(self.weights, delta_w_batchsum)]\n",
    "        self.biases = [b - (eta/len(mini_batch))*nb\n",
    "                      for b, nb in zip(self.biases, delta_b_batchsum)]\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        # calculate the updates to each weight and bias using backprop\n",
    "        # x is input from training, y is output from training\n",
    "        delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all activations, layer by layer\n",
    "        zs = [] # list to store weighted sums, z is before activation\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            # we do have a feedforward function, but the feedforward functio\n",
    "            # just returns the output of the network, but we want the \n",
    "            # activations and z's per layer to do backprop\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y)*sigmoid_prime(zs[-1])\n",
    "        delta_b[-1] = delta\n",
    "        delta_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta)*sp\n",
    "            delta_b = delta\n",
    "            delta_w = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (delta_b, delta_w)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                       for (x,y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        # cost = (1/2)*(output - y)^2\n",
    "        # cost derivative therefore is just difference\n",
    "        return (output_activations - y)\n",
    "\n",
    "#### Misc. functions\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.9+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
