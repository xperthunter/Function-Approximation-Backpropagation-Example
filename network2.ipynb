{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# network2.py\n",
    "# ~~~~~~~~~~\n",
    "#\n",
    "# Code directly from Michael Nielsen - \"Neural Networks and Deep Learning\"\n",
    "# This is the module to implement stochastic gradient descent. \n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "    # sizes is a list/array that contains number of neurons per layer. \n",
    "        # Example: sizes = [2, 3, 1]. Three layers, first with 2 neurons ...\n",
    "        # The first layer is assumed to be an input layer. \n",
    "        # The weights and biases are initialized randomly from Standard Normal here. \n",
    "        # The input layer has no biases, just layers we are feed-forwarding into. \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        # weight matrix dimension = next layer neurons, previous layer neurons\n",
    "        # its sizes[:-1] because that contains previous layer, in other words, not last layer\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                       for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        # returns the activations for each unit in network for input `a`\n",
    "        # also returns the zs for each unit\n",
    "        zs = []\n",
    "        activations = [np.array([a])]\n",
    "        z=0\n",
    "        for i in range((self.num_layers-1)):\n",
    "            z = np.dot(self.weights[i], a) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            if i < (self.num_layers-2):\n",
    "                a = vectorize_ReLu(z)\n",
    "                activations.append(a)\n",
    "            else:\n",
    "                activations.append(z)\n",
    "        return activations, zs\n",
    "    \n",
    "    def feedforwardEvaluate(self, a):\n",
    "        for i in range((self.num_layers-1)):\n",
    "            z = np.dot(self.weights[i], a) + self.biases[i]\n",
    "            if i < (self.num_layers-2):\n",
    "                a = vectorize_ReLu(z)\n",
    "        return z\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        # Here we are doing the stochastic gradient descent using mini-batches. \n",
    "        # If test_data is provided (different from training data) the partial \n",
    "        # progress at each epoch will be evaluated. Default is none. \n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0,n,mini_batch_size)\n",
    "            ]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: Insample: {1}, Outsample {2}\".format(\n",
    "                j, self.in_sample_cost(training_data), self.evaluate(test_data)))\n",
    "            else: \n",
    "                print(\"Epoch {0} complete: Insample: {1}\".format(j,\n",
    "                self.in_sample_cost(training_data)))\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        # update the network's weights and biases by applying gradient\n",
    "        # descent per batch. \n",
    "        delta_b_batchsum = [np.zeros(b.shape) for b in self.biases]\n",
    "        delta_w_batchsum = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_b_single, delta_w_single = self.backprop(x, y)\n",
    "            delta_b_batchsum = [nb+dnb for nb, dnb in zip(delta_b_batchsum, delta_b_single)]\n",
    "            delta_w_batchsum = [nw+dnw for nw, dnw in zip(delta_w_batchsum, delta_w_single)]\n",
    "        self.weights = [w - (eta/len(mini_batch))*nw\n",
    "                       for w, nw in zip(self.weights, delta_w_batchsum)]\n",
    "        self.biases = [b - (eta/len(mini_batch))*nb\n",
    "                      for b, nb in zip(self.biases, delta_b_batchsum)]\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        # calculate the updates to each weight and bias using backprop\n",
    "        # x is input from training, y is output from training\n",
    "        delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activations, zs = self.feedforward(x)\n",
    "        # backward pass\n",
    "        # deltaL, delta at last layer is just derivative of cost with respect to network output\n",
    "        # the activation is identity, so derivative is just 1. \n",
    "        delta = self.cost_derivative(activations[-1], y)*1.0\n",
    "        delta_b[-1] = delta\n",
    "        delta_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = vectorize_ReLu_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta)*sp\n",
    "            delta_b[-l] = delta\n",
    "            delta_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return delta_b, delta_w\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(self.feedforwardEvaluate(x), y)\n",
    "                       for (x,y) in test_data]\n",
    "        totalcost = sum(0.5*(x-y)**2 for (x, y) in test_results)\n",
    "        totalcost = totalcost/len(test_data)\n",
    "        return totalcost\n",
    "    \n",
    "    def in_sample_cost(self, training_data):\n",
    "        # take mini-batch and compute in sample loss, this should go down??\n",
    "        network_output = [(self.feedforwardEvaluate(x), y)\n",
    "                         for (x,y) in training_data]\n",
    "        insample_cost = sum(0.5*(x-y)**2 for (x, y) in network_output)\n",
    "        insample_cost = insample_cost/len(training_data)\n",
    "        return insample_cost\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        # cost = (1/2)*(output - y)^2\n",
    "        # cost derivative therefore is just difference\n",
    "        return (output_activations - y)\n",
    "\n",
    "#### Misc. functions\n",
    "def ReLu(z):\n",
    "    if z > 0: \n",
    "        return z\n",
    "    else:\n",
    "        return 0.0\n",
    "vectorize_ReLu = np.vectorize(ReLu)\n",
    "\n",
    "def ReLu_prime(z):\n",
    "    if z > 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "vectorize_ReLu_prime = np.vectorize(ReLu_prime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
